{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c984a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:17.926376Z",
     "iopub.status.busy": "2026-01-23T08:13:17.925726Z",
     "iopub.status.idle": "2026-01-23T08:13:47.173987Z",
     "shell.execute_reply": "2026-01-23T08:13:47.173069Z"
    },
    "papermill": {
     "duration": 29.253577,
     "end_time": "2026-01-23T08:13:47.175612",
     "exception": false,
     "start_time": "2026-01-23T08:13:17.922035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-kem9syot\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-kem9syot\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting ftfy (from clip==1.0)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\r\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=97b54d9e9f037021b3954b3ff3a30390d6b2edc4b26c1d02e81f3be05321337c\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4bh1us9x/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: ftfy, clip\r\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\r\n",
      "Collecting facenet-pytorch\r\n",
      "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: facenet-pytorch\r\n",
      "Successfully installed facenet-pytorch-2.6.0\r\n",
      "✅ PyTorch Version: 2.8.0+cu126\n",
      "✅ CUDA Available: True\n",
      "   GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# 1. Install OpenAI CLIP (safe)\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# 2. Install Facenet-PyTorch WITHOUT dependencies\n",
    "# This prevents it from uninstalling your GPU-enabled PyTorch\n",
    "!pip install facenet-pytorch --no-deps\n",
    "\n",
    "# 3. Verify imports immediately to catch errors early\n",
    "import torch\n",
    "import clip\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "print(f\"✅ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"✅ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ WARNING: You are running on CPU! Enable 'GPU T4 x2' in Accelerator settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d27add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:47.183462Z",
     "iopub.status.busy": "2026-01-23T08:13:47.182854Z",
     "iopub.status.idle": "2026-01-23T08:13:47.186375Z",
     "shell.execute_reply": "2026-01-23T08:13:47.185699Z"
    },
    "papermill": {
     "duration": 0.008975,
     "end_time": "2026-01-23T08:13:47.187746",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.178771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"pillow<10.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce449d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:47.194446Z",
     "iopub.status.busy": "2026-01-23T08:13:47.193832Z",
     "iopub.status.idle": "2026-01-23T08:13:47.197037Z",
     "shell.execute_reply": "2026-01-23T08:13:47.196373Z"
    },
    "papermill": {
     "duration": 0.007891,
     "end_time": "2026-01-23T08:13:47.198441",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.190550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be47fdd6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:47.204726Z",
     "iopub.status.busy": "2026-01-23T08:13:47.204345Z",
     "iopub.status.idle": "2026-01-23T08:13:47.208615Z",
     "shell.execute_reply": "2026-01-23T08:13:47.207962Z"
    },
    "papermill": {
     "duration": 0.008882,
     "end_time": "2026-01-23T08:13:47.209925",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.201043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from facenet_pytorch import MTCNN\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94991d",
   "metadata": {
    "papermill": {
     "duration": 0.002751,
     "end_time": "2026-01-23T08:13:47.215523",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.212772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9f3096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:47.221642Z",
     "iopub.status.busy": "2026-01-23T08:13:47.221445Z",
     "iopub.status.idle": "2026-01-23T08:13:47.225234Z",
     "shell.execute_reply": "2026-01-23T08:13:47.224579Z"
    },
    "papermill": {
     "duration": 0.008688,
     "end_time": "2026-01-23T08:13:47.226789",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.218101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = \"/kaggle/input/flickrfaceshq-dataset-ffhq\"\n",
    "OUTPUT_PATH = \"/kaggle/working/ffhq_features_augmented.pt\"\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634f646e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:13:47.233837Z",
     "iopub.status.busy": "2026-01-23T08:13:47.233415Z",
     "iopub.status.idle": "2026-01-23T08:15:38.174691Z",
     "shell.execute_reply": "2026-01-23T08:15:38.173862Z"
    },
    "papermill": {
     "duration": 110.949347,
     "end_time": "2026-01-23T08:15:38.179007",
     "exception": false,
     "start_time": "2026-01-23T08:13:47.229660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ROOT path: /kaggle/input/flickrfaceshq-dataset-ffhq\n",
      "✅ Path exists.\n",
      "Contents of root: ['27147.png', '52235.png', '32352.png', '41695.png', '21130.png', '36145.png', '22897.png', '18966.png', '22069.png', '47434.png']\n",
      "\n",
      "Searching for images...\n",
      "Found 52001 PNGs\n",
      "Found 0 JPGs\n",
      "Total Images: 52001\n",
      "\n",
      "✅ Images found! The path is correct.\n",
      "Sample image path: /kaggle/input/flickrfaceshq-dataset-ffhq/27147.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Verify the Root Path\n",
    "print(f\"Checking ROOT path: {DATASET_ROOT}\")\n",
    "\n",
    "if os.path.exists(DATASET_ROOT):\n",
    "    print(\"✅ Path exists.\")\n",
    "    print(f\"Contents of root: {os.listdir(DATASET_ROOT)[:10]}\") # Show first 10 items\n",
    "else:\n",
    "    print(\"❌ Path does NOT exist. Please verify the dataset is attached.\")\n",
    "\n",
    "# 2. Check for Images (Recursive Search)\n",
    "print(\"\\nSearching for images...\")\n",
    "pngs = glob.glob(os.path.join(DATASET_ROOT, \"**/*.png\"), recursive=True)\n",
    "jpgs = glob.glob(os.path.join(DATASET_ROOT, \"**/*.jpg\"), recursive=True)\n",
    "total_images = len(pngs) + len(jpgs)\n",
    "\n",
    "print(f\"Found {len(pngs)} PNGs\")\n",
    "print(f\"Found {len(jpgs)} JPGs\")\n",
    "print(f\"Total Images: {total_images}\")\n",
    "\n",
    "if total_images == 0:\n",
    "    print(\"\\n⚠️ PROBLEM FOUND: No images found.\")\n",
    "    print(\"Try changing DATASET_ROOT to one of the subfolders printed in step 1.\")\n",
    "else:\n",
    "    print(\"\\n✅ Images found! The path is correct.\")\n",
    "    print(f\"Sample image path: {pngs[0] if pngs else jpgs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a6a3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:15:38.186475Z",
     "iopub.status.busy": "2026-01-23T08:15:38.186161Z",
     "iopub.status.idle": "2026-01-23T08:15:38.190904Z",
     "shell.execute_reply": "2026-01-23T08:15:38.190266Z"
    },
    "papermill": {
     "duration": 0.010349,
     "end_time": "2026-01-23T08:15:38.192284",
     "exception": false,
     "start_time": "2026-01-23T08:15:38.181935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- HELPER: CLAHE  ---\n",
    "def apply_clahe(image_pil):\n",
    "    \"\"\"Applies Contrast Limited Adaptive Histogram Equalization.\"\"\"\n",
    "    # Convert PIL to CV2 (RGB -> BGR)\n",
    "    img_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Split channels, apply CLAHE to L channel of LAB color space (standard practice)\n",
    "    lab = cv2.cvtColor(img_cv, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l,a,b))\n",
    "    img_cv = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return Image.fromarray(img_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e4d994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:15:38.199024Z",
     "iopub.status.busy": "2026-01-23T08:15:38.198799Z",
     "iopub.status.idle": "2026-01-23T08:15:38.209045Z",
     "shell.execute_reply": "2026-01-23T08:15:38.208452Z"
    },
    "papermill": {
     "duration": 0.015283,
     "end_time": "2026-01-23T08:15:38.210420",
     "exception": false,
     "start_time": "2026-01-23T08:15:38.195137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "import io\n",
    "\n",
    "# --- NEW: Augmentation Helper (Section E of Thesis) ---\n",
    "def apply_thesis_augmentations(img_pil):\n",
    "    \"\"\"\n",
    "    Applies random JPEG compression, resizing, and color jitter \n",
    "    as specified in Section E of the thesis.\n",
    "    \"\"\"\n",
    "    # 1. Random Color Jitter (Brightness, Contrast, Saturation)\n",
    "    # We use mild factors to keep the face recognizable\n",
    "    if random.random() < 0.5:\n",
    "        enhancer = ImageEnhance.Brightness(img_pil)\n",
    "        img_pil = enhancer.enhance(random.uniform(0.8, 1.2))\n",
    "        enhancer = ImageEnhance.Contrast(img_pil)\n",
    "        img_pil = enhancer.enhance(random.uniform(0.8, 1.2))\n",
    "\n",
    "    # 2. Random Resizing (Upscaling/Downscaling simulation)\n",
    "    if random.random() < 0.5:\n",
    "        orig_size = img_pil.size\n",
    "        # Downscale to random size between 50% and 90%\n",
    "        scale = random.uniform(0.5, 0.9)\n",
    "        new_size = (int(orig_size[0]*scale), int(orig_size[1]*scale))\n",
    "        img_pil = img_pil.resize(new_size, Image.BILINEAR)\n",
    "        # Upscale back to 336 (Simulates super-resolution artifacting or blur)\n",
    "        img_pil = img_pil.resize((336, 336), Image.BICUBIC)\n",
    "\n",
    "    # 3. JPEG Compression (Quality 50-95)\n",
    "    if random.random() < 0.5:\n",
    "        output_io = io.BytesIO()\n",
    "        # Random quality between 50 and 95\n",
    "        q = random.randint(50, 95)\n",
    "        img_pil.save(output_io, \"JPEG\", quality=q)\n",
    "        output_io.seek(0)\n",
    "        img_pil = Image.open(output_io)\n",
    "    \n",
    "    return img_pil\n",
    "\n",
    "# --- UPDATED DATASET CLASS ---\n",
    "class ThesisDataset(Dataset):\n",
    "    def __init__(self, root_dir, clip_preprocess, augment=True): # Added augment flag\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, \"**/*.png\"), recursive=True) + \\\n",
    "                           glob.glob(os.path.join(root_dir, \"**/*.jpg\"), recursive=True)\n",
    "        self.preprocess = clip_preprocess\n",
    "        self.mtcnn = MTCNN(keep_all=False, select_largest=True, device=DEVICE)\n",
    "        self.augment = augment \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            \n",
    "            # 1. Detect & Crop\n",
    "            boxes, _ = self.mtcnn.detect(img)\n",
    "            if boxes is None: return None\n",
    "            \n",
    "            box = [int(b) for b in boxes[0]]\n",
    "            face_img = img.crop(box)\n",
    "            face_img = face_img.resize((336, 336), Image.BICUBIC)\n",
    "            \n",
    "            # 2. Apply CLAHE (Standard Preprocessing)\n",
    "            face_clean = apply_clahe(face_img)\n",
    "            \n",
    "            # 3. Prepare Tensors\n",
    "            tensors = []\n",
    "            \n",
    "            # Tensor A: The Clean Real Face\n",
    "            tensors.append(self.preprocess(face_clean))\n",
    "            \n",
    "            # Tensor B: The Augmented Real Face (For Robustness)\n",
    "            if self.augment:\n",
    "                face_aug = apply_thesis_augmentations(face_clean) # Apply Jitter/JPEG\n",
    "                tensors.append(self.preprocess(face_aug))\n",
    "            \n",
    "            # Return stacked tensors (2, 3, 336, 336) or just (1, ...)\n",
    "            return torch.stack(tensors) \n",
    "\n",
    "        except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac43c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:15:38.217127Z",
     "iopub.status.busy": "2026-01-23T08:15:38.216903Z",
     "iopub.status.idle": "2026-01-23T10:16:20.806841Z",
     "shell.execute_reply": "2026-01-23T10:16:20.806169Z"
    },
    "papermill": {
     "duration": 7242.595114,
     "end_time": "2026-01-23T10:16:20.808355",
     "exception": false,
     "start_time": "2026-01-23T08:15:38.213241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP ViT-L/14@336px...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 891M/891M [00:05<00:00, 184MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52001 images. Starting extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 813/813 [1:59:28<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features shape: torch.Size([103982, 768])\n",
      "✅ Saved to /kaggle/working/ffhq_features_augmented.pt\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXTRACTION LOOP ---\n",
    "def extract_features():\n",
    "    # 1. Load CLIP Model \n",
    "    print(\"Loading CLIP ViT-L/14@336px...\")\n",
    "    model, preprocess = clip.load(\"ViT-L/14@336px\", device=DEVICE)\n",
    "    model.eval() # Freeze weights\n",
    "\n",
    "    # 2. Prepare Dataset\n",
    "    dataset = ThesisDataset(DATASET_ROOT, preprocess)\n",
    "    \n",
    "    # collate_fn to filter out None (images where no face was found)\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x is not None, batch))\n",
    "        return torch.stack(batch) if len(batch) > 0 else None\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Found {len(dataset)} images. Starting extraction...\")\n",
    "\n",
    "    all_features = []\n",
    "    \n",
    "    # 3. Extraction Loop\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            if batch is None: continue\n",
    "            \n",
    "            b, n_augs, c, h, w = batch.shape\n",
    "            batch = batch.view(b * n_augs, c, h, w)\n",
    "            \n",
    "            batch = batch.to(DEVICE)\n",
    "            \n",
    "            features = model.encode_image(batch)\n",
    "            features /= features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            all_features.append(features.cpu())\n",
    "\n",
    "    # 4. Save Final Tensor\n",
    "    if all_features:\n",
    "        final_tensor = torch.cat(all_features, dim=0)\n",
    "        print(f\"Saving features shape: {final_tensor.shape}\")\n",
    "        torch.save(final_tensor, OUTPUT_PATH)\n",
    "        print(f\"✅ Saved to {OUTPUT_PATH}\")\n",
    "    else:\n",
    "        print(\"❌ No features extracted. Check dataset path.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e2e1f",
   "metadata": {
    "papermill": {
     "duration": 0.032021,
     "end_time": "2026-01-23T10:16:20.874391",
     "exception": false,
     "start_time": "2026-01-23T10:16:20.842370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1749d",
   "metadata": {
    "papermill": {
     "duration": 0.034446,
     "end_time": "2026-01-23T10:16:20.940675",
     "exception": false,
     "start_time": "2026-01-23T10:16:20.906229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 546691,
     "sourceId": 997012,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7390.561024,
   "end_time": "2026-01-23T10:16:24.561009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-23T08:13:13.999985",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
