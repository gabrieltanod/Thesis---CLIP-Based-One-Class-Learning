{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 14461442,
          "sourceType": "datasetVersion",
          "datasetId": 9236786
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrieltanod/Thesis---CLIP-Based-One-Class-Learning/blob/main/thesis_one_class_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. Install OpenAI CLIP (safe)\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# # 2. Install Facenet-PyTorch WITHOUT dependencies\n",
        "# # This prevents it from uninstalling your GPU-enabled PyTorch\n",
        "# !pip install facenet-pytorch --no-deps\n",
        "\n",
        "# # 3. Verify imports immediately to catch errors early\n",
        "# import torch\n",
        "# import clip\n",
        "# from facenet_pytorch import MTCNN\n",
        "\n",
        "# print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
        "# print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
        "# if torch.cuda.is_available():\n",
        "#     print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "# else:\n",
        "#     print(\"‚ùå WARNING: You are running on CPU! Enable 'GPU T4 x2' in Accelerator settings.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.853780Z",
          "iopub.execute_input": "2026-01-11T08:10:05.854456Z",
          "iopub.status.idle": "2026-01-11T08:10:05.857692Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.854428Z",
          "shell.execute_reply": "2026-01-11T08:10:05.856996Z"
        },
        "id": "M0NHMt61Wiv6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Use the Augmented Features from Notebook 1\n",
        "PATH_REAL_FEATURES = \"/content/ffhq_features.pt\"\n",
        "PATH_FAKE_FEATURES = \"/content/test_fake_dalle.pt\"\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 50\n",
        "ALPHA = 0.5 # Weighting factor (0.5 = Equal weight to SVDD and Recon)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Running on: {DEVICE}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.859144Z",
          "iopub.execute_input": "2026-01-11T08:10:05.859437Z",
          "iopub.status.idle": "2026-01-11T08:10:05.878996Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.859416Z",
          "shell.execute_reply": "2026-01-11T08:10:05.878321Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se_lZ8D3Wiv7",
        "outputId": "44ec0a7c-16c4-49f2-ce1f-f370c15475e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. THE HYBRID MODEL ---\n",
        "class HybridOneClassModel(nn.Module):\n",
        "    def __init__(self, input_dim=768, svdd_latent=64, recon_hidden=384):\n",
        "        super(HybridOneClassModel, self).__init__()\n",
        "\n",
        "        # HEAD 1: Deep SVDD (Compresses to compact manifold)\n",
        "        self.svdd_encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256, bias=False),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, svdd_latent, bias=False)\n",
        "        )\n",
        "\n",
        "        # HEAD 2: Reconstruction (Shallow Decoder: 768 -> Hidden -> 768)\n",
        "        # As per thesis: \"Shallow decoder\" to learn real feature correlations\n",
        "        self.reconstructor = nn.Sequential(\n",
        "            nn.Linear(input_dim, recon_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(recon_hidden, recon_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(recon_hidden, input_dim) # Output matches input (z_hat)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Path A: SVDD Projection\n",
        "        z_svdd = self.svdd_encoder(x)\n",
        "\n",
        "        # Path B: Reconstruction\n",
        "        z_hat = self.reconstructor(x)\n",
        "\n",
        "        return z_svdd, z_hat"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.879757Z",
          "iopub.execute_input": "2026-01-11T08:10:05.879960Z",
          "iopub.status.idle": "2026-01-11T08:10:05.894567Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.879940Z",
          "shell.execute_reply": "2026-01-11T08:10:05.894036Z"
        },
        "id": "A_-doDz5Wiv8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. THE TRAINER ---\n",
        "class HybridTrainer:\n",
        "    def __init__(self, model, center, optimizer, alpha=0.5):\n",
        "        self.model = model\n",
        "        self.center = center.detach() # Fixed center c\n",
        "        self.optimizer = optimizer\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        loss_svdd_log = 0\n",
        "        loss_recon_log = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            z_svdd, z_hat = self.model(x)\n",
        "\n",
        "            # --- LOSS CALCULATION ---\n",
        "            # 1. SVDD Loss: Distance to center ||z_svdd - c||^2\n",
        "            loss_svdd = torch.mean(torch.sum((z_svdd - self.center) ** 2, dim=1))\n",
        "\n",
        "            # 2. Reconstruction Loss: ||x - x_hat||^2\n",
        "            loss_recon = torch.mean(torch.sum((x - z_hat) ** 2, dim=1))\n",
        "\n",
        "            # Combined Loss\n",
        "            loss = loss_svdd + loss_recon\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss_svdd_log += loss_svdd.item()\n",
        "            loss_recon_log += loss_recon.item()\n",
        "\n",
        "        return total_loss / len(dataloader), loss_svdd_log / len(dataloader), loss_recon_log / len(dataloader)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.895423Z",
          "iopub.execute_input": "2026-01-11T08:10:05.895654Z",
          "iopub.status.idle": "2026-01-11T08:10:05.914966Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.895632Z",
          "shell.execute_reply": "2026-01-11T08:10:05.914329Z"
        },
        "id": "T-EMMUymWiv8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. UTILS ---\n",
        "def init_center_c(model, dataloader):\n",
        "    \"\"\"Initialize SVDD center c from the mean of initial pass.\"\"\"\n",
        "    print(\"Initializing center c...\")\n",
        "    model.eval()\n",
        "    vectors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(DEVICE)\n",
        "            z_svdd, _ = model(x) # Only care about SVDD head for center\n",
        "            vectors.append(z_svdd)\n",
        "\n",
        "    c = torch.mean(torch.cat(vectors), dim=0)\n",
        "    # Prevent collapse to zero\n",
        "    c[(abs(c) < 0.1) & (c < 0)] = -0.1\n",
        "    c[(abs(c) < 0.1) & (c > 0)] = 0.1\n",
        "    return c\n",
        "\n",
        "def get_anomaly_score(model, x, c, alpha):\n",
        "    \"\"\"Computes s = alpha * ||z-c||^2 + (1-alpha) * ||x - x_hat||^2\"\"\"\n",
        "    z_svdd, z_hat = model(x)\n",
        "\n",
        "    # Score 1: SVDD Distance\n",
        "    score_svdd = torch.sum((z_svdd - c)**2, dim=1)\n",
        "\n",
        "    # Score 2: Reconstruction Error\n",
        "    score_recon = torch.sum((x - z_hat)**2, dim=1)\n",
        "\n",
        "    # Combined Score\n",
        "    return (alpha * score_svdd) + ((1 - alpha) * score_recon)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.916263Z",
          "iopub.execute_input": "2026-01-11T08:10:05.916803Z",
          "iopub.status.idle": "2026-01-11T08:10:05.931926Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.916769Z",
          "shell.execute_reply": "2026-01-11T08:10:05.931301Z"
        },
        "id": "s1956LP8Wiv8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    # --- A. DATA LOADING & SPLITTING ---\n",
        "    print(\"Loading Features...\")\n",
        "    # Load Real Data (FFHQ)\n",
        "    full_real_tensor = torch.load(PATH_REAL_FEATURES).float()\n",
        "\n",
        "    # 80/20 Split: 80% for Training/Val, 20% reserved for Final Testing\n",
        "    total_count = len(full_real_tensor)\n",
        "    train_val_size = int(0.8 * total_count)\n",
        "    test_real_size = total_count - train_val_size\n",
        "\n",
        "    train_val_data, test_real_data = random_split(\n",
        "        TensorDataset(full_real_tensor), [train_val_size, test_real_size]\n",
        "    )\n",
        "\n",
        "    # Further split Train/Val (90% Train, 10% Val for Thresholding)\n",
        "    train_size = int(0.9 * len(train_val_data))\n",
        "    val_size = len(train_val_data) - train_size\n",
        "    train_data, val_data = random_split(train_val_data, [train_size, val_size])\n",
        "\n",
        "    # Load Fake Data (Used ONLY for Testing)\n",
        "    fake_tensor = torch.load(PATH_FAKE_FEATURES).float()\n",
        "\n",
        "    print(f\"SPLIT -> Train: {len(train_data)} | Val: {len(val_data)} | Test Real: {len(test_real_data)} | Test Fake: {len(fake_tensor)}\")\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- B. TRAINING ---\n",
        "    model = HybridOneClassModel().to(DEVICE)\n",
        "    c = init_center_c(model, train_loader) # Initialize Center\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\nStarting Training (One-Class Learning)...\")\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            x = batch[0].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            z_svdd, z_hat = model(x)\n",
        "\n",
        "            # Loss = SVDD Distance + Reconstruction Error\n",
        "            loss_svdd = torch.mean(torch.sum((z_svdd - c) ** 2, dim=1))\n",
        "            loss_recon = torch.mean(torch.sum((x - z_hat) ** 2, dim=1))\n",
        "            loss = loss_svdd + loss_recon\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # --- C. THRESHOLDING (Validation Phase) ---\n",
        "    # Thesis: \"Thresholded at validation FAR=5%\"\n",
        "    print(\"\\nCalculating Threshold (FAR=5%)...\")\n",
        "    model.eval()\n",
        "    val_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            s = get_anomaly_score(model, batch[0].to(DEVICE), c, ALPHA)\n",
        "            val_scores.append(s.cpu())\n",
        "    val_scores = torch.cat(val_scores).numpy()\n",
        "\n",
        "    # 95th Percentile: 95% of real images are below this score.\n",
        "    # Any score higher than this is flagged as Fake.\n",
        "    threshold = np.percentile(val_scores, 95)\n",
        "    print(f\" learned Threshold: {threshold:.4f}\")\n",
        "\n",
        "    # --- D. FINAL TESTING (Inference) ---\n",
        "    print(\"\\nRunning Final Inference...\")\n",
        "\n",
        "    # 1. Score Real Test Set\n",
        "    test_real_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(test_real_data, batch_size=BATCH_SIZE):\n",
        "            s = get_anomaly_score(model, batch[0].to(DEVICE), c, ALPHA)\n",
        "            test_real_scores.append(s.cpu())\n",
        "    test_real_scores = torch.cat(test_real_scores).numpy()\n",
        "\n",
        "    # 2. Score Fake Test Set\n",
        "    fake_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(TensorDataset(fake_tensor), batch_size=BATCH_SIZE):\n",
        "            s = get_anomaly_score(model, batch[0].to(DEVICE), c, ALPHA)\n",
        "            fake_scores.append(s.cpu())\n",
        "    fake_scores = torch.cat(fake_scores).numpy()\n",
        "\n",
        "    # 3. Calculate AUC\n",
        "    y_true = [0] * len(test_real_scores) + [1] * len(fake_scores)\n",
        "    y_scores = np.concatenate([test_real_scores, fake_scores])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "    print(f\"\\nüéØ FINAL RESULT (AUC): {auc:.4f}\")\n",
        "\n",
        "    # 4. Plot\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(test_real_scores, bins=50, alpha=0.7, label='Real (Test)')\n",
        "    plt.hist(fake_scores, bins=50, alpha=0.7, label='Fake (OOD)')\n",
        "    plt.axvline(threshold, color='red', linestyle='--', label='Threshold (FAR=5%)')\n",
        "    plt.title(f\"Detection Performance (AUC={auc:.4f})\")\n",
        "    plt.xlabel(\"Anomaly Score (Higher = More Likely Fake)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # --- RETURN VARIABLES SO WE CAN USE THEM LATER ---\n",
        "    return model, c, test_real_scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.932691Z",
          "iopub.execute_input": "2026-01-11T08:10:05.932978Z",
          "iopub.status.idle": "2026-01-11T08:10:05.956286Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.932939Z",
          "shell.execute_reply": "2026-01-11T08:10:05.955749Z"
        },
        "id": "Nm_BxMjQWiv8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Capture the trained model, center, and real scores\n",
        "    model, c, test_real_scores = run_experiment()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T08:10:05.971541Z",
          "iopub.execute_input": "2026-01-11T08:10:05.972117Z",
          "iopub.status.idle": "2026-01-11T08:10:54.432395Z",
          "shell.execute_reply.started": "2026-01-11T08:10:05.972096Z",
          "shell.execute_reply": "2026-01-11T08:10:54.431647Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "collapsed": true,
        "id": "PzBFLORYWiv9",
        "outputId": "6ff9b332-d726-4fd2-fd31-f3c17b90347b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/ffhq_features.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-513022758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Capture the trained model, center, and real scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_real_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2866681305.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load Real Data (FFHQ)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfull_real_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_REAL_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 80/20 Split: 80% for Training/Val, 20% reserved for Final Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ffhq_features.pt'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# DOUBLE CHECK THESE PATHS! If you are on Kaggle, use /kaggle/working/...\n",
        "FAKE_DATASETS = {\n",
        "    \"DALL-E (GenImage)\": \"/content/test_fake_dalle.pt\",\n",
        "    \"Stable Diffusion\": \"/content/test_fake_stablediffusion.pt\",\n",
        "    \"OpenJourney\": \"/content/test_fake_openjourney.pt\"\n",
        "}\n",
        "\n",
        "thesis_results = {}\n",
        "\n",
        "print(f\"‚ö° STARTING VISUAL STRESS TEST ON {len(FAKE_DATASETS)} DATASETS\\n\")\n",
        "\n",
        "# Ensure model is in eval mode\n",
        "model.eval()\n",
        "\n",
        "# 1. Create the Main ROC Figure explicitly\n",
        "fig_roc = plt.figure(figsize=(10, 8))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
        "plt.xlabel('False Positive Rate (FAR)')\n",
        "plt.ylabel('True Positive Rate (TAR)')\n",
        "plt.title('Cross-Generator Generalization (ROC Curves)')\n",
        "\n",
        "colors = ['darkorange', 'green', 'purple', 'red', 'cyan']\n",
        "color_idx = 0\n",
        "valid_plot_count = 0\n",
        "\n",
        "for name, path in FAKE_DATASETS.items():\n",
        "    print(f\"--- Processing: {name} ---\")\n",
        "\n",
        "    try:\n",
        "        # Load Fake Data\n",
        "        fake_tensor = torch.load(path).float()\n",
        "\n",
        "        # Score Fake Data\n",
        "        fake_scores = []\n",
        "        with torch.no_grad():\n",
        "            for batch in DataLoader(TensorDataset(fake_tensor), batch_size=BATCH_SIZE):\n",
        "                s = get_anomaly_score(model, batch[0].to(DEVICE), c, ALPHA)\n",
        "                fake_scores.append(s.cpu())\n",
        "        fake_scores = torch.cat(fake_scores).numpy()\n",
        "\n",
        "        # Metrics\n",
        "        y_true = [0] * len(test_real_scores) + [1] * len(fake_scores)\n",
        "        y_scores = np.concatenate([test_real_scores, fake_scores])\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        thesis_results[name] = roc_auc\n",
        "\n",
        "        print(f\"   üëâ AUC: {roc_auc:.4f}\")\n",
        "\n",
        "        # --- PLOT 1: Add to Combined ROC (Targeting fig_roc) ---\n",
        "        plt.figure(fig_roc.number) # Switch focus back to ROC figure\n",
        "        plt.plot(fpr, tpr, lw=2, color=colors[color_idx % len(colors)],\n",
        "                 label=f'{name} (AUC = {roc_auc:.4f})')\n",
        "        color_idx += 1\n",
        "        valid_plot_count += 1\n",
        "\n",
        "        # --- PLOT 2: Individual Histogram ---\n",
        "        # Create a new figure just for this histogram\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.hist(test_real_scores, bins=50, alpha=0.6, label='Real (Test)', density=True)\n",
        "        plt.hist(fake_scores, bins=50, alpha=0.6, label=f'Fake ({name})', density=True)\n",
        "\n",
        "        try:\n",
        "            plt.axvline(threshold, color='red', linestyle='--', label='Threshold (FAR=5%)')\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "        plt.title(f'Distribution: Real vs {name}')\n",
        "        plt.xlabel('Anomaly Score')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Cleanup\n",
        "        del fake_tensor, fake_scores, y_true, y_scores, fpr, tpr\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"   ‚ö†Ô∏è File not found: {path} (Skipping)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error processing {name}: {e}\")\n",
        "\n",
        "# --- FINALIZE COMBINED ROC ---\n",
        "plt.figure(fig_roc.number) # Reactivate ROC figure\n",
        "if valid_plot_count > 0:\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚ùå No data was plotted. Please check your file paths in FAKE_DATASETS.\")\n",
        "    plt.close()\n",
        "\n",
        "# --- PRINT TABLE ---\n",
        "if thesis_results:\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä FINAL THESIS RESULTS TABLE\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{'Generator':<30} | {'AUC Score':<10}\")\n",
        "    print(\"-\" * 43)\n",
        "    for name, score in thesis_results.items():\n",
        "        print(f\"{name:<30} | {score:.4f}\")\n",
        "    print(\"=\"*40)"
      ],
      "metadata": {
        "id": "zG_-HSHWcN6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "FAKE_DATASETS = {\n",
        "    \"DALL-E\": \"/content/test_fake_dalle.pt\",\n",
        "    \"Stable Diffusion\": \"/content/test_fake_stablediffusion.pt\",\n",
        "    \"OpenJourney\": \"/content/test_fake_openjourney.pt\"\n",
        "}\n",
        "\n",
        "# Check if 'threshold' exists (from training cell). If not, define it manually or rerun training.\n",
        "if 'threshold' not in locals():\n",
        "    print(\"‚ö†Ô∏è Threshold variable not found. Please run the training/inference cell first!\")\n",
        "else:\n",
        "    print(f\"‚ö° USING THRESHOLD: {threshold:.4f} (Derived from Real Validation Set)\")\n",
        "\n",
        "    # Setup the figure for side-by-side matrices\n",
        "    n_datasets = len(FAKE_DATASETS)\n",
        "    fig, axes = plt.subplots(1, n_datasets, figsize=(6 * n_datasets, 5))\n",
        "\n",
        "    # Ensure axes is iterable if there's only one dataset\n",
        "    if n_datasets == 1: axes = [axes]\n",
        "\n",
        "    # Model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    for i, (name, path) in enumerate(FAKE_DATASETS.items()):\n",
        "        print(f\"Generating matrix for: {name}...\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load Fake Data\n",
        "            fake_tensor = torch.load(path).float()\n",
        "\n",
        "            # 2. Score Fake Data\n",
        "            fake_scores = []\n",
        "            with torch.no_grad():\n",
        "                for batch in DataLoader(TensorDataset(fake_tensor), batch_size=BATCH_SIZE):\n",
        "                    s = get_anomaly_score(model, batch[0].to(DEVICE), c, ALPHA)\n",
        "                    fake_scores.append(s.cpu())\n",
        "            fake_scores = torch.cat(fake_scores).numpy()\n",
        "\n",
        "            # 3. Create Binary Labels & Predictions\n",
        "            # Real Data (Class 0)\n",
        "            real_true = [0] * len(test_real_scores)\n",
        "            real_pred = (test_real_scores > threshold).astype(int) # 0=Real, 1=Fake prediction\n",
        "\n",
        "            # Fake Data (Class 1)\n",
        "            fake_true = [1] * len(fake_scores)\n",
        "            fake_pred = (fake_scores > threshold).astype(int)      # 0=Real, 1=Fake prediction\n",
        "\n",
        "            # Combine\n",
        "            y_true = np.concatenate([real_true, fake_true])\n",
        "            y_pred = np.concatenate([real_pred, fake_pred])\n",
        "\n",
        "            # 4. Generate Matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "            # 5. Plot\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "            disp.plot(cmap='Blues', ax=axes[i], values_format='d', colorbar=False)\n",
        "\n",
        "            axes[i].set_title(f\"{name}\\n(Acc: {accuracy_score(y_true, y_pred):.2%})\")\n",
        "\n",
        "            # Cleanup\n",
        "            del fake_tensor, fake_scores, y_true, y_pred\n",
        "            gc.collect()\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            axes[i].text(0.5, 0.5, \"File Not Found\", ha='center', va='center')\n",
        "            print(f\"‚ö†Ô∏è File not found: {path}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O6Bl8mjswQ-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}